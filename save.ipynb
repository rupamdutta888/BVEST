{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the limit on the number of rows displayed by pandas\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Read csv files in pandas dataframe\n",
    "testDf = pd.read_csv('../input/test.csv')\n",
    "trainDf = pd.read_csv('../input/train.csv')\n",
    "print(\"Training dataset basic information\")\n",
    "print(\"Rows: {}\".format(len(trainDf)))\n",
    "print(\"Columns: {}\".format(len(trainDf.columns)))\n",
    "trainDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test dataset basic information\")\n",
    "print(\"Rows: {}\".format(len(testDf)))\n",
    "print(\"Columns: {}\".format(len(testDf.columns)))\n",
    "testDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add null Target column to test\n",
    "testDf['Target'] = np.nan\n",
    "data = trainDf.append(testDf, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find missing values in training and test dataset\n",
    "def findColumnsWithNan(df):\n",
    "    cols = df.columns[df.isna().any()]\n",
    "    print(\"Number of columns with Nan: {}\".format(len(cols)))\n",
    "    print(\"Column names: {}\".format(cols))\n",
    "    print(\"-\" * 80)\n",
    "    for col in cols:\n",
    "        print(\"Column: [{}] missing {} values.\".format(col, len(df[df[col].isna() == True])))\n",
    "\n",
    "print(\"Analysis of training dataset...\")\n",
    "findColumnsWithNan(trainDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Analysis of test dataset...\")\n",
    "findColumnsWithNan(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['tipovivi1'] == 1) & (data['v2a1'].isna()), 'v2a1'] = 0\n",
    "print(\"Missing values after replacing: {}\".format(len(data.loc[data['v2a1'].isna()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['v18q1'].isna(), 'v18q1'] = 0\n",
    "print(\"Missing values after replacing: {}\".format(len(data.loc[data['v18q1'].isna()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['age'] < 7) & (data['rez_esc'].isna()), 'rez_esc'] = 0\n",
    "data.loc[(data['age'] > 19) & (data['rez_esc'].isna()), 'rez_esc'] = 0\n",
    "print(\"Missing values after replacing: {}\".format(len(data.loc[data['rez_esc'].isna()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['age'] < 19 & data['meaneduc'].isna(), 'meaneduc'] = 0\n",
    "print(\"Missing values after replacing: {}\".format(len(data.loc[data['meaneduc'].isna()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('SQBmeaned', inplace=True, axis=1)\n",
    "print(\"Total number of columns left: {}\".format(len(data.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in data.columns[1:]:\n",
    "    if cols in ['idhogar', 'dependency', 'edjefe', 'edjefa']:\n",
    "        continue\n",
    "    percentile75 = np.percentile(data[cols].fillna(0), 75)\n",
    "    percentile25 = np.percentile(data[cols].fillna(0), 25)\n",
    "    threshold = (percentile75 - percentile25) * 1.5\n",
    "    lower, upper = (percentile25 - threshold), (percentile75 + threshold)\n",
    "    # identify outliers\n",
    "    outliers = data.loc[(data[cols] < lower) & (data[cols] > upper)]\n",
    "    if len(outliers) > 0:\n",
    "        print('Feature: {}. Identified outliers: {}'.format(cols, len(outliers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['dependency', 'edjefe', 'edjefa']:\n",
    "    data.loc[data[col] == 'yes', col] = 1.0\n",
    "    data.loc[data[col] == 'no', col] = 0.0\n",
    "    data[col] = pd.to_numeric(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMat = data.corr()\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.heatmap(corrMat.iloc[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuresToDrop(corrMatrix):\n",
    "    \"\"\"\n",
    "    To remove correlated features, used this gem of a code from here:\n",
    "    https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features\n",
    "    \"\"\"\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corrMatrix.where(np.triu(np.ones(corrMatrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    return [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "toDrop = featuresToDrop(corrMat)\n",
    "data.drop(toDrop, inplace=True, axis=1)\n",
    "print(\"Correlated features which are dropped: {}\".format(toDrop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(data.drop(columns = ['Id', 'idhogar', 'Target']).columns)\n",
    "aggDf = data.drop(columns='Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std'])\n",
    "# Rename the columns\n",
    "new_col = []\n",
    "for c in aggDf.columns.levels[0]:\n",
    "    for stat in aggDf.columns.levels[1]:\n",
    "        new_col.append('{}-{}'.format(c, stat))\n",
    "        \n",
    "aggDf.columns = new_col\n",
    "toDrop = featuresToDrop(aggDf.corr())\n",
    "aggDf.drop(toDrop, inplace=True, axis=1)\n",
    "data = data.merge(aggDf, on='idhogar', how ='left')\n",
    "print('Training feature shape: ', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['phones-per-capita'] = data['qmobilephone'] / data['tamviv']\n",
    "data['tablets-per-capita'] = data['v18q1'] / data['tamviv']\n",
    "data['rooms-per-capita'] = data['rooms'] / data['tamviv']\n",
    "data['rent-per-capita'] = data['v2a1'] / data['tamviv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for training\n",
    "trainTarget = np.array(list(data[data['Target'].notnull()]['Target'].astype(np.uint8)))\n",
    "submission = data.loc[data['Target'].isnull(), 'Id'].to_frame()\n",
    "\n",
    "# Extract the training data\n",
    "trainData = data[data['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "testData = data[data['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "\n",
    "# Impute training and test data\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "trainData = imputer.fit_transform(trainData)\n",
    "testData = imputer.transform(testData)\n",
    "\n",
    "# Scale training and test data\n",
    "scaler = MinMaxScaler()\n",
    "trainData = scaler.fit_transform(trainData)\n",
    "testData = scaler.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=10, \n",
    "                               n_jobs = -1)\n",
    "scorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')\n",
    "# 10 fold cross validation\n",
    "cv_score = cross_val_score(model, trainData, trainTarget, cv=10, scoring=scorer)\n",
    "print('10 Fold Cross Validation F1 Score = {} with std = {}'.format(round(cv_score.mean(), 4), round(cv_score.std(), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 5\n",
    "kf = StratifiedKFold(n_splits=kfold, shuffle=True)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=10, \n",
    "                               n_jobs = -1)\n",
    "predicts_result = []\n",
    "for idx, (train_index, test_index) in enumerate(kf.split(trainData, trainTarget)):\n",
    "    print(\"Fold: {}\".format(idx + 1))\n",
    "    X_train, X_val = trainData[train_index], trainData[test_index]\n",
    "    y_train, y_val = trainTarget[train_index], trainTarget[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    predicts_result.append(model.predict(testData))\n",
    "submission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lightgbm.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n",
    "                             random_state=None, silent=True, metric='None', \n",
    "                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n",
    "                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)\n",
    "kfold = 5\n",
    "kf = StratifiedKFold(n_splits=kfold, shuffle=True)\n",
    "\n",
    "predicts_result = []\n",
    "for idx, (train_index, test_index) in enumerate(kf.split(trainData, trainTarget)):\n",
    "    print(\"Fold: {}\".format(idx + 1))\n",
    "    X_train, X_val = trainData[train_index], trainData[test_index]\n",
    "    y_train, y_val = trainTarget[train_index], trainTarget[test_index]\n",
    "    model.fit(X_train, y_train, verbose=100)\n",
    "    predicts_result.append(model.predict(testData))\n",
    "submission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
